# Dataset / Model parameters
npl_depth: 2
data_dir: /data/imagenet
dataset: ImageFolder
train_split: train
val_split: val
model: swin_3dtrl_tiny_patch4_window7_224 
pretrained: false 
initial_checkpoint: ''
resume: '' 
no_resume_opt: false
num_classes: 1000
gp: 'avg'
img_size: 224
input_size: null
crop_pct: 0.9
mean:
- 0.485
- 0.456
- 0.406 
std:
- 0.229
- 0.224
- 0.225 
interpolation: 'bicubic' 
batch_size: 128 
validation_batch_size_multiplier: 1 

# Optimizer parameters
opt: adamw
opt_eps: 1e-8 
opt_betas: null
momentum: 0.9
weight_decay: 0.05 
clip_grad: 5.0 
clip_mode: norm

# Learning rate schedule parameters
sched: cosine
lr: 1e-3 
lr_noise: null
lr_noise_pct: 0.67
lr_noise_std: 1.0
lr_cycle_mul: 1.0
lr_cycle_limit: 1
warmup_lr: 1e-6 
min_lr: 1e-5 
epochs: 300 
epoch_repeats: 0.0
start_epoch: null
decay_epochs: 30 
warmup_epochs: 20 
cooldown_epochs: 10
patience_epochs: 10
decay_rate: 0.1 

# Augmentation & regularization parameters
no_aug: false
scale:
- 0.08
- 1.0
ratio:
- 0.67
- 1.5
hflip: 0.5
vflip: 0.0
color_jitter: 0.4
aa: rand-m9-mstd0.5-inc1 
aug_splits: 0
jsd: false
reprob: 0.25 
remode: pixel
recount: 1 
resplit: false
mixup: 0 
cutmix: 0
cutmix_minmax: null
mixup_prob: 1.0
mixup_switch_prob: 0.5
mixup_mode: batch
mixup_off_epoch: 0
smoothing: 0.1
train_interpolation: random
drop: 0. 
drop_connect: null
drop_path: 0.1 
drop_block: null

# Batch norm parameters (only works with gen_efficientnet based models currently)
bn_tf: false
bn_momentum: null
bn_eps: null
sync_bn: true 
dist_bn: '' 
split_bn: false

# Model Exponential Moving Average
model_ema: false 
model_ema_force_cpu: false
model_ema_decay: 0.99992

# Misc
seed: 42
log_interval: 500
recovery_interval: 0
checkpoint_hist: 10
workers: 16 
save_images: false
amp: false
apex_amp: false
native_amp: true
channels_last: false
pin_mem: false
no_prefetcher: false
output: 'runs'
experiment: ''
eval_metric: top1
tta: 0
local_rank: 0
use_multi_epochs_loader: true 
torchscript: false
log_wandb: false